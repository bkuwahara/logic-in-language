{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory and load API key \n",
    "import os\n",
    "os.chdir(\"/w/246/ikozlov/csc2542-project/\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads the environment variables from a .env file.\n",
    "\n",
    "# Get OpenAI API Key\n",
    "from openai import OpenAI \n",
    "client = OpenAI() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function for Calling Model with a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epistemic_logic import is_entailment\n",
    "import json\n",
    "\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"is_entailment\",\n",
    "                \"description\": '''Get as output the words 'entailment' or 'non-entailment' \n",
    "                                to find out whether the given hypothesis is entailed or not entailed by the premise. \n",
    "                                Note that premise and hypothesis need to be converted to symbolic epistemic logic, where \n",
    "                                $$B(Agent,p)$$ is used to denote that Agent believes p and $$K(Agent,p)$$ to denote that Agent knows p and \n",
    "                                p ^ q is usd to denote p and q. For example the sentence \"Anna believes the Bob knows that the sky is blue\" \n",
    "                                may be written in symbolic epistemic logic by letting p be \"Sky is Blue\" and then writing the sentence as \n",
    "                                \"$$B(Anna, K(Bob, p))$$\"''',\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"premise\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": '''The premise of the problem in epistemic logic format encapsulted by the symbols $$, \n",
    "                            e.g. $$K(Anna, B(Bob, p))$$''',\n",
    "                        },\n",
    "                        \"hypothesis\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": '''The hypothesis in the given problem in epistemic logic format, \n",
    "                            e.g. $$B(Marry, p)$$''',\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"premise\", \"hypothesis\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def gpt_sym(model, messages, prompt): \n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "                model = model, \n",
    "                messages = messages, \n",
    "                tools = tools, \n",
    "                tool_choice = \"auto\"\n",
    "            )\n",
    "    \n",
    "    response_message = completion.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    function_call = False \n",
    "    if tool_calls: \n",
    "        function_call = True\n",
    "        model_output = \"\"\n",
    "        for tool_call in tool_calls: \n",
    "            premise = \"\" \n",
    "            hypothesis = \"\"\n",
    "            try: \n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                premise += function_args.get(\"premise\")\n",
    "                hypothesis += function_args.get(\"hypothesis\") \n",
    "                reasoning_output = is_entailment(premise, hypothesis)\n",
    "                model_output += f\"Function arguments: Premise: {premise}, Hypothesis: {hypothesis}, Answer: {reasoning_output}. \"\n",
    "            except: \n",
    "\n",
    "                model_output += f\"Function arguments: Premise: {premise}, Hypothesis: {hypothesis}. \"\n",
    "    else: \n",
    "        model_output = response_message.content \n",
    "\n",
    "    return model_output, function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function for Testing GPT on Given Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import json \n",
    "import random \n",
    "\n",
    "output_dir = \"results\"\n",
    "\n",
    "def test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix = None):\n",
    "    # Load dataset \n",
    "    with open(f\"./datasets/{dataset}.json\") as data_file: \n",
    "        data = json.load(data_file) \n",
    "\n",
    "    prefix = data[\"task_prefix\"] if prefix == None else prefix \n",
    "    questions = data[\"examples\"]\n",
    "\n",
    "    # Set up output file for data\n",
    "    savedir = f\"./{output_dir}/{model_name}/{model}\"\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    output_file = f\"{savedir}/{dataset}_{n_shots}shot\"\n",
    "    if chain_of_thought and (n_shots > 0):\n",
    "        output_file += \"_cot\"\n",
    "    output_file += \".csv\"\n",
    "\n",
    "    # Start writing CSV\n",
    "    with open(output_file, 'w', newline='') as output:\n",
    "        writer = csv.writer(output)\n",
    "        header = [\"question_index\", \"response\", \"is_correct\"]\n",
    "        writer.writerow(header)\n",
    "\n",
    "    # Get prompt \n",
    "    if model_name == \"basic\":\n",
    "        prompt_name = f'respond_{n_shots}shot' + ('_cot' if chain_of_thought else \"\")\n",
    "    elif model_name == \"logical\": \n",
    "        prompt_name = f'translate_gpt_{n_shots}shot' + ('_cot' if chain_of_thought else \"\")\n",
    "    else: \n",
    "        raise ValueError(\"Wrong model name: \", model_name)\n",
    "\n",
    "    if n_shots != 0: \n",
    "        with open(f\"./prompts/{prompt_name}.txt\", 'r') as f:\n",
    "            prompt = f.read()\n",
    "    else: \n",
    "        prompt = \"\"\n",
    "\n",
    "    # Pattern to match to parse model output \n",
    "    pattern = r\"Answer: ([a-zA-Z-]+)\"\n",
    "\n",
    "    # Compute score \n",
    "    scores = []\n",
    "    failures = []\n",
    "    function_calls = []\n",
    "    questions = random.sample(questions, 500)\n",
    "    num_runs = len(questions)\n",
    "    for i in tqdm(range(0,num_runs)):\n",
    "        if model_name == \"basic\": \n",
    "            completion = client.chat.completions.create(\n",
    "                model = model, \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\":prefix},\n",
    "                    {\"role\": \"user\", \"content\":prompt + questions[i][\"input\"]}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            model_output = completion.choices[0].message.content\n",
    "\n",
    "        elif model_name == \"logical\": \n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\":prefix + (prompt if (n_shots != 0) else \"\")},\n",
    "                    {\"role\": \"user\", \"content\":questions[i][\"input\"]}\n",
    "                ]\n",
    "            model_output, function_call = gpt_sym(model, messages, prompt)\n",
    "            function_calls.append(function_call)\n",
    "\n",
    "        else: \n",
    "            raise ValueError(\"Wrong model name: \", model_name)\n",
    "\n",
    "        try: \n",
    "            response = re.search(pattern, model_output).group(1)\n",
    "        except: \n",
    "            response = model_output\n",
    "\n",
    "        response = response.lower()\n",
    "        is_invalid = response not in [\"entailment\", \"non-entailment\"]\n",
    "        failures.append(is_invalid)\n",
    "\n",
    "        scores.append(float('NaN') if is_invalid else questions[i][\"target_scores\"][response])\n",
    "\n",
    "        with open(output_file, 'a', newline='') as output:\n",
    "            writer = csv.writer(output)\n",
    "            writer.writerow([i, model_output, scores[-1]])\n",
    "\n",
    "    accuracy = np.nanmean(np.array(scores)) \n",
    "    failure_rate = np.mean(np.array(failures))\n",
    "    \n",
    "    if model_name == \"basic\":\n",
    "        return accuracy, failure_rate\n",
    "    else: \n",
    "        function_calls = np.array(function_calls)\n",
    "        function_call_rate = np.mean(function_calls)\n",
    "        accurate_func_call_rate = np.nansum(function_calls == np.array(scores))/np.sum(function_calls)\n",
    "        return accuracy, failure_rate, function_call_rate, accurate_func_call_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 0-shot GPT-4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [06:09<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.722\n",
      "Failure Rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 0\n",
    "dataset = \"task\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Indicate the response after the word 'Answer: '\\n\"\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix=prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical 0-shot GPT-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [17:47<00:00,  2.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9044715447154471\n",
      "Failure Rate:  0.016\n",
      "Function call rate:  0.808\n",
      "Accuracy at function calls:  0.8935643564356436\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 0\n",
    "dataset = \"task\"\n",
    "model_name = \"logical\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Indicate the response after the word 'Answer: '\\n\"\n",
    "accuracy, failure_rate, function_call_rate, accurate_func_call_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix=prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)\n",
    "print(\"Function call rate: \", function_call_rate)\n",
    "print(\"Accuracy at function calls: \", accurate_func_call_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic GPT-3.5-Turbo Model Tests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [19:07<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6082164328657315\n",
      "Failure Rate:  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 3\n",
    "dataset = \"task\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████████████████                                                         | 1088/2000 [1:02:52<52:07,  3.43s/it]"
     ]
    }
   ],
   "source": [
    "chain_of_thought = True\n",
    "n_shots = 3\n",
    "dataset = \"task\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [04:55<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.616\n",
      "Failure Rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 3\n",
    "dataset = \"mixed_reasoning\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\n\"\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix = prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [29:13<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6895833333333333\n",
      "Failure Rate:  0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = True\n",
    "n_shots = 3\n",
    "dataset = \"mixed_reasoning\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\n\"\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix = prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai-env)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
