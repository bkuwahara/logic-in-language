{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to project directory and load API key \n",
    "import os\n",
    "os.chdir(\"/w/246/ikozlov/csc2542-project/\")\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()  # This loads the environment variables from a .env file.\n",
    "\n",
    "# Get OpenAI API Key\n",
    "from openai import OpenAI \n",
    "client = OpenAI() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function for Calling Model with a Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from epistemic_logic import is_entailment\n",
    "import json\n",
    "\n",
    "tools = [\n",
    "        {\n",
    "            \"type\": \"function\",\n",
    "            \"function\": {\n",
    "                \"name\": \"is_entailment\",\n",
    "                \"description\": '''Get as output the words 'entailment' or 'non-entailment' \n",
    "                                to find out whether the given hypothesis is entailed or not entailed by the premise. \n",
    "                                Note that premise and hypothesis need to be converted to symbolic epistemic logic, where \n",
    "                                $$B(Agent,p)$$ is used to denote that Agent believes p and $$K(Agent,p)$$ to denote that Agent knows p and \n",
    "                                p ^ q is usd to denote p and q. For example the sentence \"Anna believes the Bob knows that the sky is blue\" \n",
    "                                may be written in symbolic epistemic logic by letting p be \"Sky is Blue\" and then writing the sentence as \n",
    "                                \"$$B(Anna, K(Bob, p))$$\"''',\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"premise\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": '''The premise of the problem in epistemic logic format encapsulted by the symbols $$, \n",
    "                            e.g. $$K(Anna, B(Bob, p))$$''',\n",
    "                        },\n",
    "                        \"hypothesis\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": '''The hypothesis in the given problem in epistemic logic format, \n",
    "                            e.g. $$B(Marry, p)$$''',\n",
    "                        },\n",
    "                    },\n",
    "                    \"required\": [\"premise\", \"hypothesis\"],\n",
    "                },\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "def gpt_sym(model, messages, prompt): \n",
    "    \n",
    "    completion = client.chat.completions.create(\n",
    "                model = model, \n",
    "                messages = messages, \n",
    "                tools = tools, \n",
    "                tool_choice = \"auto\"\n",
    "            )\n",
    "    \n",
    "    response_message = completion.choices[0].message\n",
    "    tool_calls = response_message.tool_calls\n",
    "\n",
    "    function_call = False \n",
    "    if tool_calls: \n",
    "        function_call = True\n",
    "        model_output = \"\"\n",
    "        for tool_call in tool_calls: \n",
    "            premise = \"\" \n",
    "            hypothesis = \"\"\n",
    "            try: \n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                premise += function_args.get(\"premise\")\n",
    "                hypothesis += function_args.get(\"hypothesis\") \n",
    "                reasoning_output = is_entailment(premise, hypothesis)\n",
    "                model_output += f\"Function arguments: Premise: {premise}, Hypothesis: {hypothesis}, Answer: {reasoning_output}. \"\n",
    "            except: \n",
    "\n",
    "                model_output += f\"Function arguments: Premise: {premise}, Hypothesis: {hypothesis}. \"\n",
    "    else: \n",
    "        model_output = response_message.content \n",
    "\n",
    "    return model_output, tool_calls, function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Function for Testing GPT on Given Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "import json \n",
    "import random \n",
    "\n",
    "output_dir = \"results\"\n",
    "\n",
    "def test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix = None):\n",
    "    # Load dataset \n",
    "    with open(f\"./datasets/{dataset}.json\") as data_file: \n",
    "        data = json.load(data_file) \n",
    "\n",
    "    prefix = data[\"task_prefix\"] if prefix == None else prefix \n",
    "    questions = data[\"examples\"]\n",
    "\n",
    "    # Set up output file for data\n",
    "    savedir = f\"./{output_dir}/{model_name}/{model}\"\n",
    "    if not os.path.exists(savedir):\n",
    "        os.makedirs(savedir)\n",
    "    output_file = f\"{savedir}/{dataset}_{n_shots}shot\"\n",
    "    if chain_of_thought and (n_shots > 0):\n",
    "        output_file += \"_cot\"\n",
    "    output_file += \".csv\"\n",
    "\n",
    "    # Start writing CSV\n",
    "    with open(output_file, 'w', newline='') as output:\n",
    "        writer = csv.writer(output)\n",
    "        header = [\"question_index\", \"response\", \"is_correct\"]\n",
    "        writer.writerow(header)\n",
    "\n",
    "    # Get prompt \n",
    "    if model_name == \"basic\":\n",
    "        prompt_name = f'respond_{n_shots}shot' + ('_cot' if chain_of_thought else \"\")\n",
    "    elif model_name == \"logical\": \n",
    "        prompt_name = f'translate_gpt_{n_shots}shot' + ('_cot' if chain_of_thought else \"\")\n",
    "    else: \n",
    "        raise ValueError(\"Wrong model name: \", model_name)\n",
    "\n",
    "    if n_shots != 0: \n",
    "        with open(f\"./prompts/{prompt_name}.txt\", 'r') as f:\n",
    "            prompt = f.read()\n",
    "    else: \n",
    "        prompt = \"\"\n",
    "\n",
    "    # Pattern to match to parse model output \n",
    "    pattern = r\"Answer: ([a-zA-Z-]+)\"\n",
    "\n",
    "    # Compute score \n",
    "    scores = []\n",
    "    failures = []\n",
    "    function_calls = []\n",
    "    questions = random.sample(questions, 500)\n",
    "    num_runs = len(questions)\n",
    "    for i in tqdm(range(0,num_runs)):\n",
    "        if model_name == \"basic\": \n",
    "            completion = client.chat.completions.create(\n",
    "                model = model, \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\":prefix},\n",
    "                    {\"role\": \"user\", \"content\":prompt + questions[i][\"input\"]}\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            model_output = completion.choices[0].message.content\n",
    "\n",
    "        elif model_name == \"logical\": \n",
    "            messages=[\n",
    "                    {\"role\": \"system\", \"content\":prefix + (prompt if (n_shots != 0) else \"\")},\n",
    "                    {\"role\": \"user\", \"content\":questions[i][\"input\"]}\n",
    "                ]\n",
    "            model_output, function_call = gpt_sym(model, messages, prompt)\n",
    "            function_calls.append(function_call)\n",
    "\n",
    "        else: \n",
    "            raise ValueError(\"Wrong model name: \", model_name)\n",
    "\n",
    "        try: \n",
    "            response = re.search(pattern, model_output).group(1)\n",
    "        except: \n",
    "            response = model_output\n",
    "\n",
    "        response = response.lower()\n",
    "        is_invalid = response not in [\"entailment\", \"non-entailment\"]\n",
    "        failures.append(is_invalid)\n",
    "\n",
    "        scores.append(float('NaN') if is_invalid else questions[i][\"target_scores\"][response])\n",
    "\n",
    "        with open(output_file, 'a', newline='') as output:\n",
    "            writer = csv.writer(output)\n",
    "            writer.writerow([i, model_output, scores[-1]])\n",
    "\n",
    "    accuracy = np.nanmean(np.array(scores)) \n",
    "    failure_rate = np.mean(np.array(failures))\n",
    "    \n",
    "    if model_name == \"basic\":\n",
    "        return accuracy, failure_rate\n",
    "    else: \n",
    "        function_calls = np.array(function_calls)\n",
    "        function_call_rate = np.mean(function_calls)\n",
    "        accurate_func_call_rate = np.nansum(function_calls == np.array(scores))/np.sum(function_calls)\n",
    "        return accuracy, failure_rate, function_call_rate, accurate_func_call_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline 0-shot GPT-4: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                         | 0/500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [06:09<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.722\n",
      "Failure Rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 0\n",
    "dataset = \"task\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Indicate the response after the word 'Answer: '\\n\"\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix=prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logical 0-shot GPT-4:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|████▍                                                                                           | 23/500 [00:42<15:56,  2.00s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|█████████████████████████████████████████████████████████████▌                                 | 324/500 [11:12<06:05,  2.07s/it]\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Invalid \\escape: line 2 column 37 (char 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIdentify the relation between the following premises and hypotheses, choosing from the options \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentailment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon-entailment\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Indicate the response after the word \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAnswer: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m accuracy, failure_rate, function_call_rate, accurate_func_call_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtest_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_shots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_of_thought\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailure Rate: \u001b[39m\u001b[38;5;124m\"\u001b[39m, failure_rate)\n",
      "Cell \u001b[0;32mIn[76], line 73\u001b[0m, in \u001b[0;36mtest_gpt\u001b[0;34m(dataset, model, model_name, n_shots, chain_of_thought, prefix)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogical\u001b[39m\u001b[38;5;124m\"\u001b[39m: \n\u001b[1;32m     69\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m     70\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:prefix \u001b[38;5;241m+\u001b[39m (prompt \u001b[38;5;28;01mif\u001b[39;00m (n_shots \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)},\n\u001b[1;32m     71\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m:questions[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m     72\u001b[0m         ]\n\u001b[0;32m---> 73\u001b[0m     model_output, function_call \u001b[38;5;241m=\u001b[39m \u001b[43mgpt_sym\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     function_calls\u001b[38;5;241m.\u001b[39mappend(function_call)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n",
      "Cell \u001b[0;32mIn[75], line 53\u001b[0m, in \u001b[0;36mgpt_sym\u001b[0;34m(model, messages, prompt)\u001b[0m\n\u001b[1;32m     51\u001b[0m model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tool_call \u001b[38;5;129;01min\u001b[39;00m tool_calls: \n\u001b[0;32m---> 53\u001b[0m     function_args \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_call\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marguments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     premise \u001b[38;5;241m=\u001b[39m function_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpremise\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     55\u001b[0m     hypothesis \u001b[38;5;241m=\u001b[39m function_args\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhypothesis\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/openai-env/lib/python3.10/json/__init__.py:346\u001b[0m, in \u001b[0;36mloads\u001b[0;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    341\u001b[0m     s \u001b[38;5;241m=\u001b[39m s\u001b[38;5;241m.\u001b[39mdecode(detect_encoding(s), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    344\u001b[0m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    345\u001b[0m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[0;32m--> 346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m JSONDecoder\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/openai-env/lib/python3.10/json/decoder.py:337\u001b[0m, in \u001b[0;36mJSONDecoder.decode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, s, _w\u001b[38;5;241m=\u001b[39mWHITESPACE\u001b[38;5;241m.\u001b[39mmatch):\n\u001b[1;32m    333\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    containing a JSON document).\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \n\u001b[1;32m    336\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     end \u001b[38;5;241m=\u001b[39m _w(s, end)\u001b[38;5;241m.\u001b[39mend()\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m end \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[0;32m/w/246/ikozlov/miniconda3/envs/openai-env/lib/python3.10/json/decoder.py:353\u001b[0m, in \u001b[0;36mJSONDecoder.raw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Decode a JSON document from ``s`` (a ``str`` beginning with\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;124;03ma JSON document) and return a 2-tuple of the Python\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;124;03mrepresentation and the index in ``s`` where the document ended.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m \n\u001b[1;32m    351\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 353\u001b[0m     obj, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpecting value\u001b[39m\u001b[38;5;124m\"\u001b[39m, s, err\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Invalid \\escape: line 2 column 37 (char 38)"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 0\n",
    "dataset = \"task\"\n",
    "model_name = \"logical\"\n",
    "model = \"gpt-4\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'. Indicate the response after the word 'Answer: '\\n\"\n",
    "accuracy, failure_rate, function_call_rate, accurate_func_call_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix=prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)\n",
    "print(\"Function call rate: \", function_call_rate)\n",
    "print(\"Accuracy at function calls: \", accurate_func_call_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic GPT-3.5-Turbo Model Tests: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2000/2000 [19:07<00:00,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6082164328657315\n",
      "Failure Rate:  0.002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 3\n",
    "dataset = \"task\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|████████████████████████████████████████████████████████████████████                                                         | 1088/2000 [1:02:52<52:07,  3.43s/it]"
     ]
    }
   ],
   "source": [
    "chain_of_thought = True\n",
    "n_shots = 3\n",
    "dataset = \"task\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [04:55<00:00,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.616\n",
      "Failure Rate:  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = False\n",
    "n_shots = 3\n",
    "dataset = \"mixed_reasoning\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\n\"\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix = prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 500/500 [29:13<00:00,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.6895833333333333\n",
      "Failure Rate:  0.04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chain_of_thought = True\n",
    "n_shots = 3\n",
    "dataset = \"mixed_reasoning\"\n",
    "model_name = \"basic\"\n",
    "model = \"gpt-3.5-turbo\"\n",
    "\n",
    "prefix = \"Identify the relation between the following premises and hypotheses, choosing from the options 'entailment' or 'non-entailment'.\\n\"\n",
    "accuracy, failure_rate = test_gpt(dataset, model, model_name, n_shots, chain_of_thought, prefix = prefix)\n",
    "\n",
    "print(\"Accuracy: \", accuracy)\n",
    "print(\"Failure Rate: \", failure_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (openai-env)",
   "language": "python",
   "name": "openai-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
